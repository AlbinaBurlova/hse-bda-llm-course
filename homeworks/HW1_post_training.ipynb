{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AZdzG7eCj1PG",
        "OoYFKsYRpVAo",
        "wQrXQ06TsmEG",
        "nwkm-_Ccu25t",
        "aZt9DWIWvWdP",
        "2c3qnG3hwncy",
        "xzpsDZJ2w5pk",
        "Rbw3btQ2xK3k"
      ],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ДЗ1. Адаптация LLM под задачу"
      ],
      "metadata": {
        "id": "DUcoU8lBQc5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Введение"
      ],
      "metadata": {
        "id": "AZdzG7eCj1PG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дата выдачи: 08.02.2026\n",
        "\n",
        "Мягкий дедлайн: 23:59MSK 23.02.2026\n",
        "\n",
        "Жесткий дедлайн: 23:59MSK 28.02.2026"
      ],
      "metadata": {
        "id": "uyRsFaurkM0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В этом домашнем задании вам предстоит адаптировать одну и ту же LLM под задачу следования инструкциям и сравнить несколько практических подходов:\n",
        "\n",
        "* **Prompting (baseline):** без обучения, только подбор формата запроса.\n",
        "* **LoRA fine-tuning:** параметро-эффективное дообучение (можно в 4-bit режиме).\n",
        "* **Full fine-tuning:** дообучение целиком (обучаем все веса).\n",
        "* **DPO:** дообучение по предпочтениям на парных данных (chosen/rejected).\n",
        "\n",
        "В ходе работы вы пройдёте весь *post-training* пайплайн:\n",
        "\n",
        "1. Подготовка данных и шаблона диалога (chat template)\n",
        "2. Prompting baseline и набор фиксированных тест-промптов\n",
        "3. Реализация LoRA-слоя + тесты\n",
        "4. Встраивание LoRA в модель + sanity-checks\n",
        "5. SFT-дообучение: LoRA vs full fine-tuning\n",
        "6. Сравнение качества/стоимости и анализ ошибок\n",
        "7. DPO по предпочтениям (chosen/rejected)\n",
        "\n",
        "> В ноутбуке используется `tokenizer.apply_chat_template` для формирования промптов под instruct-модель.\n",
        ">\n",
        "> В TRL у тренеров аргумент `tokenizer` был переименован в processing_class (поэтому в работе используется именно он)."
      ],
      "metadata": {
        "id": "enLF1DgHovc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**О задании**\n",
        "\n",
        "Вы берете одну модель и ведете ее через этапы адаптации: prompting -> LoRA-SFT -> full-SFT -> DPO, после чего делаете сравнение на фиксированных промптах и на отложенной выборке (loss). На каждом этапе вы сохраняете результаты (ответы на одинаковых промптах и метрики), чтобы сравнение было честным и воспроизводимым.\n",
        "\n",
        "**Оценивание и условия**\n",
        "\n",
        "Максимальный балл за работу - **10**.\n",
        "\n",
        "Вот несколько правил, который помогут нам сделать работу приятнее и продуктивнее:\n",
        "\n",
        "* Можно использовать любые свободные источники с обязательным указанием ссылки на них. Если в работе вы используете генеративные модели, их указание обязательно. Также следите за оригинальностью: генеративного кода должно быть не более 60% работы. На усмотрение проверяющего баллы за работу в случае использования сгенерированного кода могут быть снижены.\n",
        "\n",
        "* Плагиат не допускается. При обнаружении случаев списывания, 0 за работу выставляется всем участникам нарушения, даже если можно установить, кто у кого списал.\n",
        "\n",
        "* Старайтесь сделать код как можно более оптимальным. В частности, будет штрафоваться использование циклов в тех случаях, когда операцию можно совершить при помощи инструментов библиотек, о которых рассказывалось в курсе.\n",
        "\n",
        "**Формат сдачи**\n",
        "\n",
        "Задания сдаются через систему **LMS**. Посылка должна содержать:\n",
        "\n",
        "- ноутбук homework-01-Username.ipynb\n",
        "где Username - ваша фамилия латиницей, без пробелов (например, homework-01-Ivanov.ipynb).\n",
        "\n"
      ],
      "metadata": {
        "id": "Y3dbcEEPoiyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting"
      ],
      "metadata": {
        "id": "OoYFKsYRpVAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT:\n",
        "# 1) СНАЧАЛА запустите ноутбук в \"CPU-режиме\" и проверьте,\n",
        "#    что базовая часть работает: импорты, загрузка датасета,\n",
        "#    токенизация, форматирование промпта, 1 короткая генерация.\n",
        "#\n",
        "# 2) ТОЛЬКО ПОСЛЕ этого подключайте GPU и грузите модель на CUDA.\n",
        "#\n",
        "# Почему так:\n",
        "# - В Colab подключение GPU начинает \"съедать\" лимит времени/ресурсов,\n",
        "#   даже когда вы просто думаете и ничего не считаете.\n",
        "# - Если сначала отладить код на CPU, вы экономите GPU-время и нервы.\n",
        "#\n",
        "# Как подключить GPU:\n",
        "# Runtime -> Change runtime type -> Hardware accelerator -> GPU"
      ],
      "metadata": {
        "id": "83wkRUrBywjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install \"transformers>=4.43\" \"datasets>=2.18\" \"accelerate>=0.33\" \"trl>=0.9\" \"bitsandbytes>=0.45\"\n",
        "\n",
        "import os, random, math, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
        "\n",
        "DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "print(\"device:\", device, \"| dtype:\", DTYPE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lvY0UKPq0AW",
        "outputId": "cea43d37-2f48-4a00-a8bc-9c8930a0fc0b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m530.9/530.9 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hdevice: cuda | dtype: torch.bfloat16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 0. Данные и вспомогательные функции (1 балл)"
      ],
      "metadata": {
        "id": "hjZdJl7nsMLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Данные:** `tatsu-lab/alpaca`\n",
        "\n",
        "В этом ДЗ мы используем датасет `tatsu-lab/alpaca`— это версия Stanford Alpaca, одного из самых известных учебных наборов для **instruction fine-tuning (SFT)**.\n",
        "\n",
        "Он содержит пары *инструкция -> ответ* (и иногда дополнительный input) в полях `instruction`, `input`, `output`.\n",
        "\n",
        "Важно понимать, что это **синтетический** датасет: инструкции и ответы были сгенерированы с помощью модели OpenAI `text-davinci-003`, поэтому данные могут быть неидеальными и местами слишком гладкими/шаблонными - это нормальная часть эксперимента.\n",
        "\n",
        "**Почему мы работаем именно с ним?**\n",
        "\n",
        "1. **Простой и понятный формат.** Удобно собрать chat-template и быстро запустить SFT без долгой возни с парсингом диалогов.\n",
        "2. **Небольшой и быстрый для обучения.** Можно брать небольшой срез (несколько тысяч примеров) и уложиться в разумное время даже на ограниченной GPU.\n",
        "3. **Хорош для честного сравнения режимов post-training.** На одном и том же наборе легко сравнить: prompting vs LoRA-SFT vs full-SFT - и увидеть trade-off качество/стоимость.\n",
        "4. **Учебная ценность.** Датасет достаточно разнообразный по типам инструкций (объяснения, списки, преобразования, простые задачи), поэтому эффект от fine-tuning обычно заметен даже при коротком обучении."
      ],
      "metadata": {
        "id": "yQIWDw3X0tJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ознакомиться с данными (0.5 балла)"
      ],
      "metadata": {
        "id": "Lh_y-ss-1BVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перед тем как обучать модель, нужно минимально понять, что именно лежит в датасете и как это будет превращаться в диалог.\n",
        "\n",
        "**Что нужно сделать:**\n",
        "\n",
        "1. Загрузить `tatsu-lab/alpaca` и вывести:\n",
        "\n",
        "   - список колонок,\n",
        "   - размер датасета,\n",
        "   - 3 случайных примера (с выводом `instruction`, `input`, `output`).\n",
        "\n",
        "2. Посчитать и вывести (в одном месте, коротко):\n",
        "\n",
        "   - долю примеров, где `input` пустой,\n",
        "   - распределение длины `output` (например, min/median/mean/max по числу символов или токенов - на ваш выбор),\n",
        "   - топ-5 самых частых первых слов в `instruction`.\n",
        "\n",
        "3. Вручную посмотреть 5 примеров и ответить текстом:\n",
        "\n",
        "   - какие типы инструкций встречаются чаще всего?\n",
        "   - есть ли странные или явно низкокачественные ответы? (2–3 предложения)"
      ],
      "metadata": {
        "id": "KH86uVOf1O3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !в сплите можно управлять размером данных\n",
        "ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
        "\n",
        "# your code here ┌(ಠ_ಠ)┘"
      ],
      "metadata": {
        "id": "WtdM_WHt1pFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# and maybe also ٩(⁎❛ᴗ❛⁎)۶"
      ],
      "metadata": {
        "id": "SoLsOYyt2tRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Какие типы инструкций встречаются чаще всего?\n",
        "Есть ли низкокачественные примеры? (2–3 предложения)\n",
        "\n",
        "`# your text here (ಠ.ಠ)`"
      ],
      "metadata": {
        "id": "iaY24IJy2X_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Вспомогательные функции и базовая загрузка модели (0.5 балла)\n"
      ],
      "metadata": {
        "id": "GTKlGmDb5eGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дальше в ДЗ вам будет постоянно нужно:\n",
        "\n",
        "1. **собирать чат-промпт** из сообщений (user / assistant / system),\n",
        "2. **быстро генерировать ответы** базовой/дообученной модели,\n",
        "3. **превращать Alpaca-пример** в готовую строку `text` для SFT (`train_ds_txt/test_ds_txt`).\n",
        "\n",
        "Чтобы не копировать один и тот же код по всему ноутбуку, мы предлагаем сделать несколько вспомогательных функций.\n",
        "\n",
        "> **Это не строго обязательно.** Если вам проще - вы можете не писать эти функции и делать всё в лоб прямо в следующих заданиях. Но с функциями будет проще поддерживать код и сравнивать результаты.\n",
        ">\n",
        "> Если вы примете решение их не реализовывать, напишите комментарий об этом для проверяющего."
      ],
      "metadata": {
        "id": "QeAfmXiy5kFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Шаг 1. Загрузите токенайзер и базовую модель** `Qwen/Qwen3-0.6B` с [HuggingFace](https://huggingface.co/Qwen/Qwen3-0.6B).\n",
        "Проверьте, что модель переводится в `eval()` и умеет генерировать (в следующем шаге).\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8x1NQSUm6FEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here (⌐■_■)\n",
        "\n",
        "# Возможно, потребуется явно выставить pad_token\n",
        "# print(\"Tokenizer pad_token:\", tokenizer.pad_token, \"| eos_token:\", tokenizer.eos_token)"
      ],
      "metadata": {
        "id": "QEBkF2m46F9R"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Шаг 2. Подготовьте функции:**\n",
        "\n",
        "- `make_chat(messages, add_generation_prompt=True)` - собирает строку промпта из списка сообщений.\n",
        "- `make_user_assistant_pair(...)` - удобный конструктор списка сообщений.\n",
        "- `generate_text(...)` - генерирует ответ модели на один запрос.\n",
        "- `alpaca_to_text(ex)` - превращает одну запись Alpaca в поле `{\"text\": ...}` для обучения."
      ],
      "metadata": {
        "id": "H8PnLDrJ6V7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat-template\n",
        "\n",
        "def make_chat(messages, add_generation_prompt: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Универсальная сборка промпта:\n",
        "    - если у токенайзера есть chat_template -> используем apply_chat_template\n",
        "    - иначе -> делаем простой fallback-формат\n",
        "\n",
        "    messages: list[dict], пример:\n",
        "      [{\"role\":\"system\",\"content\":\"...\"}, {\"role\":\"user\",\"content\":\"...\"}]\n",
        "    \"\"\"\n",
        "\n",
        "    # your code here (づ｡◕‿‿◕｡)づ\n",
        "\n",
        "    pass"
      ],
      "metadata": {
        "id": "fhekjGzZ6vip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Удобный конструктор сообщений\n",
        "\n",
        "def make_user_assistant_pair(\n",
        "    user_text: str,\n",
        "    assistant_text: str | None,\n",
        "    system_text: str | None = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Собирает список сообщений:\n",
        "    (опционально) system -> user -> (опционально) assistant\n",
        "    \"\"\"\n",
        "\n",
        "    # your code here ٩(⁎❛ᴗ❛⁎)۶\n",
        "\n",
        "    pass"
      ],
      "metadata": {
        "id": "B1te5JYO6-tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Генерация текста (один промпт в один ответ)\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_text(\n",
        "    model,\n",
        "    user_text: str,\n",
        "    system_text: str | None = None,\n",
        "    max_new_tokens=128,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9\n",
        ") -> str:\n",
        "\n",
        "    \"\"\"\n",
        "    Генерация ответа модели на один запрос.\n",
        "    Возвращает decoded строку (без спец-токенов).\n",
        "    \"\"\"\n",
        "\n",
        "    # your code here (ง •̀_•́)ง\n",
        "\n",
        "    pass"
      ],
      "metadata": {
        "id": "WZUwGj5K7I6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def alpaca_to_text(ex):\n",
        "    \"\"\"\n",
        "    Делает из Alpaca-примера поле text (готовое для SFTTrainer),\n",
        "    используя make_chat(..., add_generation_prompt=False).\n",
        "\n",
        "    Alpaca поля:\n",
        "      ex[\"instruction\"], ex.get(\"input\",\"\"), ex[\"output\"]\n",
        "    \"\"\"\n",
        "\n",
        "    # your code here ┌(ಠ_ಠ)┘\n",
        "\n",
        "    pass"
      ],
      "metadata": {
        "id": "MqWqE1R27rdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Шаг 3. Мини-проверки (sanity-check):**\n",
        "\n",
        "- напечатайте пример результата `alpaca_to_text` (первые 400–600 символов),\n",
        "- сгенерируйте ответ базовой модели на простой промпт (1-2 примера),\n",
        "- убедитесь, что ничего не падает."
      ],
      "metadata": {
        "id": "YfznQVGJ6cRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверка: генерация на базовой модели\n",
        "print(generate_text(base_model, \"Explain what overfitting is in 2 bullet points.\"))"
      ],
      "metadata": {
        "id": "LOrQJbOI7jC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверка: что alpaca_to_text возвращает строку\n",
        "ex0 = ds[0]\n",
        "tmp = alpaca_to_text(ex0)\n",
        "print(type(tmp), list(tmp.keys()))\n",
        "print(tmp[\"text\"][:600])"
      ],
      "metadata": {
        "id": "7E3PX-lv7mFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 1. Prompting baseline (0.5 балла)"
      ],
      "metadata": {
        "id": "wQrXQ06TsmEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Прежде чем обучать модель (LoRA / full SFT), нужно получить честный нулевой уровень (*baseline*): как ведёт себя та же самая модель, если мы вообще не трогаем веса и меняем только формат запроса.\n",
        "\n",
        "Это важно по двум причинам:\n",
        "\n",
        "1. **Контрольная точка для сравнения.** Если после SFT ответы стали лучше, мы должны понимать: это действительно эффект обучения, или мы могли бы получить почти то же самое простым промптингом.\n",
        "\n",
        "2. **Проверка стабильности и воспроизводимости.** Фиксированный набор тест-промптов станет вашим мини-бенчмарком, на котором вы потом сравните baseline vs LoRA-SFT vs full-SFT.\n"
      ],
      "metadata": {
        "id": "o0jg1H7H9KT_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь давайте:\n",
        "\n",
        "1. Соберем фиксированный набор тест-промптов `EVAL_PROMPTS` (минимум 15).\n",
        "   Промпты должны быть **разнотипными**: объяснение, список, преобразование текста, простой код, вопрос на здравый смысл и т.п.\n",
        "\n",
        "2. Для каждого промпта получим ответы базовой модели в двух режимах:\n",
        "\n",
        "* **без system** (обычный user - assistant)\n",
        "* **с system** (добавляем системную инструкцию, например \"кратко, структурно\")\n",
        "\n",
        "3. Сформируем результаты в таблицу `baseline_df` со столбцами:\n",
        "\n",
        "* `prompt`\n",
        "* `no_system`\n",
        "* `with_system`"
      ],
      "metadata": {
        "id": "C47s-8x1-I5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Предполагается, что из **Задания 0** у вас уже есть:\n",
        ">*Курсив*\n",
        "> `base_model`, `generate_text()` и рабочий токенайзер/модель.\n"
      ],
      "metadata": {
        "id": "NLhVj0Zm-BDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompting baseline\n",
        "\n",
        "EVAL_PROMPTS = [\n",
        "    \"Explain overfitting in 3 bullet points.\",\n",
        "    \"Write a Python function that checks if a number is prime.\",\n",
        "    \"Give a short summary of the idea of gradient descent.\",\n",
        "\n",
        "    # your code here: добавьте минимум 12 своих промптов (✿◠‿◠)\n",
        "\n",
        "]\n",
        "\n",
        "SYSTEM_INSTR = \"Отвечай кратко, структурно, без воды. Если уместно — списком.\"\n",
        "\n",
        "baseline_rows = []\n",
        "\n",
        "for p in EVAL_PROMPTS:\n",
        "\n",
        "    # your code here (ง •̀_•́)ง\n",
        "\n",
        "    pass\n",
        "\n",
        "baseline_df = pd.DataFrame(baseline_rows)"
      ],
      "metadata": {
        "id": "1Y2uiOqK9z5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here: покажите первые строки baseline_df"
      ],
      "metadata": {
        "id": "M5u8H4iF98FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напишите коротко текстом:\n",
        "\n",
        "* В каких типах промптов `SYSTEM_INSTR` реально помогает?\n",
        "* Где почти не влияет?\n",
        "\n",
        "`# your text here (ಠ.ಠ)`"
      ],
      "metadata": {
        "id": "hzaZmZV_9vlu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 2. Реализация LoRA слоя + тесты (1.5 балла)"
      ],
      "metadata": {
        "id": "nwkm-_Ccu25t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В следующих заданиях вы будете дообучать модель с LoRA. Чтобы понимать, что именно меняется в модели и почему LoRA экономит ресурсы, мы сначала реализуем LoRA **вручную** для одного базового кирпичика - `nn.Linear`.\n",
        "\n",
        "Это дает две вещи:\n",
        "\n",
        "1. **Понимание механики:** LoRA - это не магия из библиотеки, а конкретная добавка низкорангового обновления к уже обученному слою.\n",
        "2. **Контроль корректности:** тесты гарантируют, что слой ведет себя правильно (особенно важное свойство `B=0 -> слой = base`)."
      ],
      "metadata": {
        "id": "MCiSuKjr_wvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание:**\n",
        "\n",
        "Реализуйте LoRA-обертку для `nn.Linear` по формуле:\n",
        "\n",
        "$$\n",
        "y = Wx + \\frac{\\alpha}{r}\\cdot (xAB)\n",
        "$$\n",
        "\n",
        "Где:\n",
        "\n",
        "- `W` - **замороженные** веса исходного слоя\n",
        "- `A` и `B` - **обучаемые** матрицы низкого ранга (`r`)\n",
        "- при `B = 0` слой должен в точности совпадать с исходным линейным слоем\n",
        "- градиенты должны идти только в `A` и `B` (веса `W` не обучаются)"
      ],
      "metadata": {
        "id": "1oWB2LHTu5oM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA layer\n",
        "\n",
        "class LoRALinear(nn.Module):\n",
        "    def __init__(self, base: nn.Linear, r: int = 8, alpha: float = 16.0, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        assert isinstance(base, nn.Linear)\n",
        "\n",
        "        self.base = base\n",
        "        self.r = r\n",
        "        self.alpha = alpha\n",
        "        self.scaling = alpha / r\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        # 1) Freeze base weights\n",
        "        # 2) Create LoRA params A and B\n",
        "        # 3) Init A (B stays zeros to satisfy B=0 => base)\n",
        "\n",
        "        # your code here (ง •̀_•́)ง\n",
        "\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # your code here ٩(⁎❛ᴗ❛⁎)۶\n",
        "\n",
        "        pass"
      ],
      "metadata": {
        "id": "OeD4E041_Yro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Тесты (должны проходить)\n",
        "\n",
        "def test_lora_basic():\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    lin = nn.Linear(4, 3, bias=False)\n",
        "    lin.weight.data.fill_(1.0)\n",
        "\n",
        "    lora = LoRALinear(lin, r=2, alpha=2.0, dropout=0.0)\n",
        "\n",
        "    x = torch.randn(5, 4)\n",
        "    y_base = lin(x)\n",
        "    y_lora = lora(x)\n",
        "\n",
        "    # Property: B=0 => LoRA equals base layer\n",
        "    assert torch.allclose(y_base, y_lora, atol=1e-6), \"B=0 => LoRA must equal base layer\"\n",
        "\n",
        "    # Gradients should flow into A and B\n",
        "    loss = y_lora.sum()\n",
        "    loss.backward()\n",
        "    assert lora.A.grad is not None and lora.B.grad is not None, \"No grads for LoRA params\"\n",
        "\n",
        "test_lora_basic()\n",
        "print(\"LoRA tests: OK\")"
      ],
      "metadata": {
        "id": "xPdpjd4G_RCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Мини-вопрос**\n",
        "\n",
        "Почему мы инициализируем `B` нулями, а `A` - случайно? Что это даёт в начале обучения?\n",
        "\n",
        "`# your text here (ಠ.ಠ)`\n"
      ],
      "metadata": {
        "id": "AcrA0VgP_Jwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 3. Встраивание LoRA в модель + sanity-check (1 балл)"
      ],
      "metadata": {
        "id": "m5tjsttFvIqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вы уже реализовали LoRA для одного слоя. Теперь нужно сделать следующий инженерный шаг:\n",
        "\n",
        "1. **встроить LoRA внутрь реальной LLM**, заменив нужные `nn.Linear` слои в attention-блоке,\n",
        "2. убедиться, что мы действительно обучаем только LoRA-параметры, а не случайно разморозили половину модели.\n",
        "\n",
        "Это важный sanity-check перед любым обучением: иначе вы можете потратить часы GPU и не понять, *что именно* вы обучали.\n",
        "\n",
        "**Задание:**\n",
        "\n",
        "1. Загрузите модель `MODEL_NAME`, если еще нет.\n",
        "2. Встройте LoRA минимум в слои attention-проекций:\n",
        "\n",
        "   - обязательно: `q_proj`, `k_proj`, `v_proj`\n",
        "   - опционально (приветствуется): `o_proj`\n",
        "3. Посчитайте:\n",
        "\n",
        "   - число обучаемых параметров\n",
        "   - долю обучаемых параметров от общего числа\n",
        "4. Сделайте sanity-check:\n",
        "\n",
        "   - среди всех параметров с `requires_grad=True` должны быть только LoRA-матрицы `A` и `B`."
      ],
      "metadata": {
        "id": "FmMtglunBddh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Предполагается, что из Задания 0 у вас уже есть: `MODEL_NAME`, `DTYPE`, а из Задания 2 - класс `LoRALinear`."
      ],
      "metadata": {
        "id": "pxY7kJzqBYM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inject LoRA into model\n",
        "\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "def set_submodule(root: nn.Module, path: str, new_module: nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Заменяет подмодуль по строковому пути вида \"model.layers.0.self_attn.q_proj\".\n",
        "    \"\"\"\n",
        "\n",
        "    parts = path.split(\".\")\n",
        "    parent = root\n",
        "    for p in parts[:-1]:\n",
        "        parent = getattr(parent, p)\n",
        "    setattr(parent, parts[-1], new_module)\n",
        "\n",
        "def inject_lora(\n",
        "    model: nn.Module,\n",
        "    target_suffixes=(\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"),\n",
        "    r=8,\n",
        "    alpha=16,\n",
        "    dropout=0.05\n",
        "):\n",
        "\n",
        "    # your code here (♡ (´｡• ω •｡`))\n",
        "\n",
        "    return replaced"
      ],
      "metadata": {
        "id": "O3lWw7NUBFC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load fresh model for LoRA experiment\n",
        "lora_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=DTYPE\n",
        ")\n",
        "\n",
        "# Inject LoRA\n",
        "num_replaced = inject_lora(lora_model, r=8, alpha=16, dropout=0.05)\n",
        "print(\"Replaced линейных слоёв:\", num_replaced)"
      ],
      "metadata": {
        "id": "TVK6wODHA-Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count trainable params\n",
        "\n",
        "trainable = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in lora_model.parameters())\n",
        "print(\"Trainable params:\", trainable)\n",
        "print(\"Total params:\", total)\n",
        "print(\"Trainable share:\", trainable / total)"
      ],
      "metadata": {
        "id": "F3G5KvDHA8ZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity-check: only A/B are trainable\n",
        "\n",
        "bad = []\n",
        "for n, p in lora_model.named_parameters():\n",
        "    if p.requires_grad:\n",
        "\n",
        "        # your code here (⊙_⊙)\n",
        "        # Логика: если имя не заканчивается на \".A\" или \".B\" -> добавить в bad\n",
        "\n",
        "        pass\n",
        "\n",
        "print(\"Non-LoRA trainable params:\", bad[:20], \"count=\", len(bad))\n",
        "assert len(bad) == 0, \"Нашлись обучаемые параметры не из LoRA!\"\n",
        "print(\"sanity-check: OK\")"
      ],
      "metadata": {
        "id": "EYgZ6aZ9A2oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вопрос:**\n",
        "\n",
        "Почему LoRA обычно ставят именно в `q_proj/k_proj/v_proj` (и иногда `o_proj`)? Что это дает по сравнению с \"вставить LoRA куда угодно\"?\n",
        "\n",
        "`# your text here (ಠ.ಠ)`"
      ],
      "metadata": {
        "id": "3rMrqyGfAqnC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 4. SFT обучение с LoRA (2 балла)"
      ],
      "metadata": {
        "id": "aZt9DWIWvWdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дообучите LoRA-модель на `train_ds_txt` (поле `text`) и покажите, что:\n",
        "\n",
        "- training loss убывает\n",
        "- на фиксированном наборе `EVAL_PROMPTS` ответы меняются осмысленно по сравнению с baseline\n",
        "\n",
        "Можно использовать `trl.SFTTrainer` (рекомендуется) или свой тренировочный цикл."
      ],
      "metadata": {
        "id": "nC0v7qUUvYk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "lora_model.config.use_cache = False\n",
        "lora_model.train()\n",
        "\n",
        "cfg_lora = SFTConfig(\n",
        "    output_dir=\"out_sft_lora\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    max_steps=300, # можно меньше/больше\n",
        "    logging_steps=25,\n",
        "    save_strategy=\"no\",\n",
        "    fp16=(DTYPE == torch.float16)\n",
        ")\n",
        "\n",
        "trainer_lora = SFTTrainer(\n",
        "    model=lora_model,\n",
        "    args=cfg_lora,\n",
        "    train_dataset=train_ds_txt,\n",
        "    eval_dataset=test_ds_txt,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset_text_field=\"text\",\n",
        ")\n",
        "\n",
        "# your code here: trainer_lora.train() ᕙ(⇀‸↼‶)ᕗ"
      ],
      "metadata": {
        "id": "SG6McW7Avdbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here:\n",
        "\n",
        "# 1) Сгенерируйте ответы LoRA-SFT на EVAL_PROMPTS\n",
        "# 2) Сохраните табличку prompt / baseline_with_system / lora_sft\n",
        "\n",
        "# ヽ(♡‿♡)ノ\n",
        "\n",
        "lora_rows = []\n",
        "for p in EVAL_PROMPTS:\n",
        "    base_out = baseline_df[baseline_df[\"prompt\"] == p][\"with_system\"].values[0]\n",
        "    lora_out = generate_text(lora_model, p, system_text=SYSTEM_INSTR)\n",
        "    lora_rows.append({\"prompt\": p, \"baseline_with_system\": base_out, \"lora_sft\": lora_out})\n",
        "\n",
        "cmp_df = pd.DataFrame(lora_rows)\n",
        "cmp_df.head()"
      ],
      "metadata": {
        "id": "yryrcsqYvhEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here: сохранить промежуточную таблицу\n",
        "# (⌐■_■)\n",
        "\n",
        "cmp_df.to_csv(\"compare_baseline_lora.csv\", index=False)\n",
        "print(\"saved: compare_baseline_lora.csv\")"
      ],
      "metadata": {
        "id": "fnQp3x7avn1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 5. Full fine-tuning (2 балла)"
      ],
      "metadata": {
        "id": "2c3qnG3hwncy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "На baseline-промптинге мы увидели, что можно выжать из модели без обучения. Теперь задача - сделать **instruction fine-tuning (SFT)** и проверить, что:\n",
        "\n",
        "1. модель действительно обучается (loss убывает),\n",
        "2. поведение меняется осмысленно на том же наборе `EVAL_PROMPTS` (то есть улучшение - не кажется, а видно при прямом сравнении).\n",
        "\n",
        "Это ключевой этап: дальше вы будете сравнивать LoRA-SFT с full-SFT, поэтому нужно аккуратно зафиксировать результат LoRA.\n",
        "\n",
        "**Этапы:**\n",
        "1. Соберите конфиг обучения SFTConfig.\n",
        "2. Запустите SFT-обучение LoRA-модели на `train_ds_txt` (поле `text`).\n",
        "   Можно использовать `trl.SFTTrainer` (рекомендуется) или свой цикл.\n",
        "3. Покажите, что training loss убывает (достаточно лога `logging_steps` и/или графика).\n",
        "4. Сравните ответы baseline vs LoRA-SFT на `EVAL_PROMPTS`:\n",
        "\n",
        "   * используйте тот же `SYSTEM_INSTR`, что в baseline,\n",
        "   * соберите таблицу `cmp_df` со столбцами:\n",
        "\n",
        "     * `prompt`\n",
        "     * `baseline_with_system`\n",
        "     * `lora_sft`\n",
        "\n",
        "5. Покажите `cmp_df.head()` и 3–5 примеров до/после текстом или таблицей.\n",
        "\n"
      ],
      "metadata": {
        "id": "y_NcB1W-wmCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Предполагается, что из предыдущих заданий у вас уже есть:\n",
        "> `lora_model`, `tokenizer`, `train_ds_txt`, `test_ds_txt`, `EVAL_PROMPTS`, `baseline_df`, `SYSTEM_INSTR`, `generate_text()`."
      ],
      "metadata": {
        "id": "5zHbWRtFC6aq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ограничения на конфиг**\n",
        "\n",
        "Вы можете выбирать любые значения, но соблюдайте рамки:\n",
        "\n",
        "* `max_steps`: [100, 400]\n",
        "* `learning_rate`: в диапазоне [5e-5, 5e-4]\n",
        "* `per_device_train_batch_size`: 1 или 2\n",
        "* `gradient_accumulation_steps`: ≥ 4\n",
        "* `fp16`: включайте, если `DTYPE == torch.float16` (иначе выключайте)\n",
        "* `save_strategy=\"no\"` (чтобы не плодить артефакты)"
      ],
      "metadata": {
        "id": "9-znTHbYD0wq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вопрос на понимание**\n",
        "\n",
        "Перед запуском обучения ответьте:\n",
        "\n",
        "* почему вы выбрали именно такой `learning_rate` и `max_steps`?\n",
        "* что будет, если сделать `learning_rate` слишком большим?\n",
        "\n",
        "`# your text here (ಠ.ಠ)`"
      ],
      "metadata": {
        "id": "RUnP4WjZD_9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA-SFT\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# Prepare model for training\n",
        "lora_model.config.use_cache = False\n",
        "lora_model.train()\n",
        "\n",
        "# Соберите cfg_lora сами\n",
        "\n",
        "# your code here (ง •̀_•́)ง\n",
        "\n",
        "# Подсказка: параметры, которые точно нужны:\n",
        "# - output_dir\n",
        "# - per_device_train_batch_size\n",
        "# - gradient_accumulation_steps\n",
        "# - learning_rate\n",
        "# - max_steps\n",
        "# - logging_steps\n",
        "# - save_strategy\n",
        "# - fp16\n",
        "\n",
        "cfg_lora = SFTConfig(\n",
        "\n",
        "    # <YOUR CODE HERE>\n",
        "\n",
        ")\n",
        "\n",
        "trainer_lora = SFTTrainer(\n",
        "    model=lora_model,\n",
        "    args=cfg_lora,\n",
        "    train_dataset=train_ds_txt,\n",
        "    eval_dataset=test_ds_txt,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset_text_field=\"text\",\n",
        ")\n",
        "\n",
        "# your code here: запустите обучение ᕙ(⇀‸↼‶)ᕗ\n",
        "\n",
        "pass\n",
        "\n",
        "# Compare outputs on EVAL_PROMPTS (baseline vs lora_sft)\n",
        "lora_rows = []\n",
        "\n",
        "for p in EVAL_PROMPTS:\n",
        "\n",
        "    # your code here ヽ(♡‿♡)ノ\n",
        "\n",
        "    pass\n",
        "\n",
        "cmp_df = pd.DataFrame(lora_rows)"
      ],
      "metadata": {
        "id": "mgZYVrtlCl36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here: покажите несколько строк таблицы"
      ],
      "metadata": {
        "id": "oys6HEElChVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выберите 3 промпта из `EVAL_PROMPTS`, где изменение заметнее всего, и коротко ответьте:\n",
        "\n",
        "1. что стало лучше/хуже после LoRA-SFT?\n",
        "2. есть ли побочные эффекты (например: ответы стали длиннее, более шаблонными, хуже следуют инструкции)?\n",
        "\n",
        "`# your text here (ಠ.ಠ)`"
      ],
      "metadata": {
        "id": "YlnVGfMaCa25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 6. Сравнение и анализ ошибок (1 балла)"
      ],
      "metadata": {
        "id": "xzpsDZJ2w5pk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "После prompting и двух режимов обучения (LoRA-SFT и full SFT) важно сравнить модели одним и тем же числом, которое не зависит от вкуса проверяющего и не требует оценки глазами.\n",
        "\n",
        "**Eval loss и perplexity на отложенной выборке** - это простой способ измерить, насколько модель лучше угадывает правильные ответы (в том формате, в котором мы ее обучали).\n",
        "\n",
        "> Важно: loss не гарантирует, что ответы красивее или полезнее, но это честная метрика качества под ваш SFT-объектив.\n",
        "\n",
        "**Этапы:**\n",
        "\n",
        "1. Реализуйте функцию `eval_loss(model, ds, batch_size=4)`:\n",
        "\n",
        "   * проходит по датасету батчами,\n",
        "   * токенизирует поле `text`,\n",
        "   * считает `loss` модели,\n",
        "   * возвращает среднее значение loss.\n",
        "\n",
        "2. Посчитайте `base_loss`, `lora_loss`, `full_loss` на `test_ds_txt`.\n",
        "\n",
        "3. Посчитайте и выведите perplexit для каждой модели: `ppl = exp(loss)`."
      ],
      "metadata": {
        "id": "3e8soJU8EpOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "def eval_loss(model, ds, batch_size=4):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "\n",
        "    # your code here (•̀ ω •́ )✧\n",
        "\n",
        "    return float(np.mean(losses))\n",
        "\n",
        "# your code here: покажите результаты"
      ],
      "metadata": {
        "id": "OySNW2QoE5Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ответьте коротко:\n",
        "\n",
        "- Почему важно считать loss на test, а не на train?\n",
        "- Почему perplexity - это просто `exp(loss)` (в 1 предложении)?\n",
        "\n",
        "`# your text here (ಠ.ಠ)`"
      ],
      "metadata": {
        "id": "SSD_AM44EyqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Анализ ошибок\n",
        "\n",
        "Ответьте одним-двумя абзацами:\n",
        "\n",
        "1. На каких типах промптов LoRA дала улучшение сильнее, чем full fine-tuning (если такое есть)?\n",
        "2. Где full fine-tuning оказался лучше?\n",
        "3. Видите ли вы признаки переобучения (например: ответы стали слишком шаблонными/слишком длинными/ухудшилась общая адекватность)?\n",
        "4. Какой метод вы бы выбрали в реальном проекте при ограниченном бюджете GPU - и почему?\n",
        "\n",
        "**Ответ:**\n",
        "`# your text here (ಠ.ಠ)`"
      ],
      "metadata": {
        "id": "LEUQNQtrEqjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 7. DPO (1 балл)"
      ],
      "metadata": {
        "id": "Rbw3btQ2xK3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "После SFT (supervised fine-tuning) модель учится копировать \"правильные ответы\" из датасета (instruction -> answer). Но в реальной жизни часто есть другая постановка: *нам важнее не единственно верный ответ, а предпочитаемый* - например:\n",
        "\n",
        "* ответ должен быть **более вежливым**,\n",
        "* **короче / структурнее**,\n",
        "* **без опасных советов**,\n",
        "* **более полезным**.\n",
        "\n",
        "Для этого используют обучение по предпочтениям на парных данных: для одного и того же `prompt` есть два ответа:\n",
        "\n",
        "* `chosen` - предпочтительный,\n",
        "* `rejected` - менее предпочтительный.\n",
        "\n",
        "**DPO (Direct Preference Optimization)** - простой и популярный способ дообучить модель так, чтобы она чаще предпочитала `chosen` и реже - `rejected`, *не вводя отдельную reward-модель и сложный RL-процесс*.\n",
        "\n",
        "Интуитивно: мы подкручиваем вероятности токенов так, чтобы вероятность `chosen` росла относительно `rejected`.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*lK6iJMz5CGh2fo7TsDn15A.png\" alt=\"drawing\" width=\"700\"/>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LM4DQneqGojh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Что откуда брать?**\n",
        "\n",
        "В этом задании нужны 4 вещи:\n",
        "\n",
        "1. Ваша SFT-модель (желательно `lora_model` после LoRA-SFT из Задания 4) -> это будет `dpo_model`.\n",
        "\n",
        "2. Reference model (ref_model) - якорь, замороженная копия модели до DPO. -> проще всего: загрузить заново `AutoModelForCausalLM.from_pretrained(MODEL_NAME, ...)`.\n",
        "\n",
        "   Почему не брать `base_model` из памяти? Можно, но часто он уже жил в ноутбуке, и проще гарантировать чистоту новой загрузкой.\n",
        "\n",
        "3. Preference-датасет с полями `prompt/chosen/rejected`. -> возьмём `trl-lib/ultrafeedback_binarized` (Hugging Face). Он большой, поэтому берем небольшой срез, например `train[:8000]`.\n",
        "\n",
        "4. Тренер из TRL: `DPOTrainer` и конфиг `DPOConfig`. -> библиотека `trl` уже ставится у нас в Setting.\n"
      ],
      "metadata": {
        "id": "UcGxOsW2JHYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание:**\n",
        "\n",
        "1. Загрузите preference-датасет `trl-lib/ultrafeedback_binarized` и убедитесь, что у него есть (или сделайте через `map`) колонки:\n",
        "\n",
        "   - `prompt`\n",
        "   - `chosen`\n",
        "   - `rejected`\n",
        "\n",
        "2. Соберите `ref_model` (замороженная копия исходной модели) и `dpo_model` (ваша SFT-модель).\n",
        "\n",
        "3. Соберите `DPOConfig` самостоятельно (как вы делали в SFT): подберите разумные значения `learning_rate`, `max_steps`, `gradient_accumulation_steps`, `beta`.\n",
        "\n",
        "4. Запустите DPO-обучение.\n",
        "\n",
        "5. Покажите 10–15 примеров до/после:\n",
        "\n",
        "   - до - ответы вашей SFT-модели до DPO (сохраните копию или прогоните заранее),\n",
        "   - после - ответы `dpo_model` после `trainer_dpo.train()`.\n",
        "\n",
        "6. Коротко напишите вывод:\n",
        "\n",
        "   - где стало лучше (в чём именно проявилось предпочтение),\n",
        "   - где сломалось или стало хуже (типичные failure cases DPO на вашем датасете)."
      ],
      "metadata": {
        "id": "bU817cbOIs2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Важно\n",
        ">\n",
        "> * DPO обычно запускают **поверх SFT**, иначе модель может дергаться сильнее.\n",
        ">\n",
        "> * Слишком большой `beta` делает обучение агрессивным (модель может стать однообразной).\n",
        ">\n",
        "> * Слишком длинный датасет/шаги = долго. Для ДЗ достаточно небольшого числа шагов.\n"
      ],
      "metadata": {
        "id": "NqgI6fCbIl-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load preference dataset\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "from datasets import load_dataset\n",
        "\n",
        "pref_ds = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train[:8000]\")\n",
        "\n",
        "# your code here (╯°□°）╯︵ ┻━┻\n",
        "# Логика:\n",
        "# 1) вывести column_names\n",
        "# 2) если названия отличаются, сделать pref_ds = pref_ds.map(...)\n",
        "# 3) колонки должны быть строго prompt / chosen / rejected\n",
        "\n",
        "print(pref_ds)\n",
        "print(pref_ds[0])"
      ],
      "metadata": {
        "id": "mQJfMdK7H4eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build ref_model and dpo_model\n",
        "\n",
        "# ref_model - замороженная копия модели ДО DPO\n",
        "# dpo_model - ваша SFT-модель (LoRA-SFT)\n",
        "\n",
        "# your code here: (⊙_⊙)\n",
        "# Соберите ref_model и dpo_model\n",
        "# убедитесь еще, что ref_model НЕ обучается"
      ],
      "metadata": {
        "id": "ZH3vYc1oHfMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure DPO\n",
        "\n",
        "# your code here: соберите конфиг сами (ง •̀_•́)ง\n",
        "\n",
        "# Подсказка:\n",
        "# - max_steps: 100–300 (в зависимости от времени)\n",
        "# - lr: порядка 1e-6 .. 1e-5\n",
        "# - beta: 0.05 .. 0.2\n",
        "# - batch_size обычно 1\n",
        "\n",
        "cfg_dpo = DPOConfig(\n",
        "    output_dir=\"out_dpo\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps= # <YOUR CODE HERE>\n",
        "    learning_rate= # <YOUR CODE HERE>\n",
        "    max_steps= # <YOUR CODE HERE>\n",
        "    beta= # <YOUR CODE HERE>\n",
        "    logging_steps=25,\n",
        "    save_strategy=\"no\",\n",
        "    fp16=(DTYPE == torch.float16)\n",
        ")"
      ],
      "metadata": {
        "id": "3tpVcW39HXw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before/after set\n",
        "\n",
        "# your code here: выберите 10–15 промптов из pref_ds\n",
        "# (ﾉ◕ヮ◕)ﾉ*:･ﾟ✧\n",
        "\n",
        "DPO_EVAL_PROMPTS = [\n",
        "    # <YOUR CODE HERE>\n",
        "]\n",
        "\n",
        "# your code here: получите ответы \"до DPO\"\n",
        "# Подсказка: используйте generate_text(dpo_model, prompt, system_text=SYSTEM_INSTR)\n",
        "# и сохраните результаты в список\n",
        "before_rows = []"
      ],
      "metadata": {
        "id": "ml_QJvHAHPj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train DPO\n",
        "\n",
        "trainer_dpo = DPOTrainer ... # your code here: ᕙ(⇀‸↼‶)ᕗ"
      ],
      "metadata": {
        "id": "XJ6mMeOfHGIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After DPO + comparison table\n",
        "\n",
        "# your code here: получите ответы \"после DPO\" на тех же DPO_EVAL_PROMPTS\n",
        "after_rows = []\n",
        "\n",
        "# your code here: соберите табличку сравнения\n",
        "# columns: prompt / before / after\n",
        "# и покажите ее"
      ],
      "metadata": {
        "id": "9hdEHwYIG_YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вместо итога по DPO:\n",
        "Напишите несколько предложений:\n",
        "- где стало лучше и почему\n",
        "- где стало хуже/ломается\n",
        "- что бы поменяли в конфиге (beta/steps/lr), чтобы улучшить\n",
        "\n",
        "`# your text here (ಠ.ಠ)`"
      ],
      "metadata": {
        "id": "5B75WkeJGqys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сделайте DPO поверх вашей SFT-модели (**рекомендуется начинать с LoRA-SFT**).\n",
        "\n",
        "Нужно:\n",
        "\n",
        "1. Загрузить preference-датасет (prompt/chosen/rejected) ([Hugging Face][2])\n",
        "2. Запустить DPO-обучение\n",
        "3. Показать **10–15 примеров “до/после”** и короткий вывод: стало ли лучше и где ломается"
      ],
      "metadata": {
        "id": "l3Lq_tJGxN7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import DPOTrainer, DPOConfig\n",
        "\n",
        "# Пример preference-датасета (можно заменить на другой)\n",
        "pref_ds = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train[:8000]\")\n",
        "\n",
        "# your code here: проверьте колонки pref_ds и при необходимости сделайте map\n",
        "# нужно получить колонки: prompt / chosen / rejected\n",
        "# (╯°□°）╯︵ ┻━┻\n",
        "\n",
        "pref_ds"
      ],
      "metadata": {
        "id": "1as-qikRxPi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ref_model — \"референс\", обычно замороженная копия модели ДО DPO\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=DTYPE\n",
        ")\n",
        "ref_model.eval()\n",
        "\n",
        "# dpo_model — ваша SFT-модель (обычно LoRA-SFT удобнее)\n",
        "dpo_model = lora_model\n",
        "dpo_model.train()\n",
        "dpo_model.config.use_cache = False\n",
        "\n",
        "cfg_dpo = DPOConfig(\n",
        "    output_dir=\"out_dpo\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    learning_rate=5e-6,\n",
        "    max_steps=200,\n",
        "    beta=0.1,\n",
        "    logging_steps=25,\n",
        "    save_strategy=\"no\",\n",
        "    fp16=(DTYPE == torch.float16)\n",
        ")\n",
        "\n",
        "trainer_dpo = DPOTrainer(\n",
        "    model=dpo_model,\n",
        "    ref_model=ref_model,\n",
        "    args=cfg_dpo,\n",
        "    train_dataset=pref_ds,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# your code here: trainer_dpo.train() ᕙ(⇀‸↼‶)ᕗ"
      ],
      "metadata": {
        "id": "qMH6YAp9xTFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here:\n",
        "\n",
        "# 1) выберите 10–15 промптов (можно из pref_ds)\n",
        "# 2) покажите \"до DPO\" (используйте LoRA-SFT модель до обучения или baseline)\n",
        "# 3) покажите \"после DPO\" (dpo_model)\n",
        "# 4) сохраните таблицу dpo_before_after.csv\n",
        "\n",
        "# (ﾉ◕ヮ◕)ﾉ*:･ﾟ✧"
      ],
      "metadata": {
        "id": "mLDdKrJdr7MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Заключение\n",
        "\n",
        "Вы попробовали 4 подхода к адаптации одной LLM:\n",
        "\n",
        "- Prompting baseline (без обучения)\n",
        "- LoRA-SFT (parameter-efficient)\n",
        "- Full fine-tuning (все веса)\n",
        "- DPO (предпочтения)\n",
        "\n",
        "**Вопрос:** Какой метод показал лучшее соотношение **качество / сложность / время / требования к ресурсам**?\n",
        "Если бы вы делали продакшн-систему для автоматических ответов ассистента в компании (ограниченный GPU-бюджет), что бы вы выбрали и почему?\n",
        "\n",
        "`your text here ( •̀ ω •́ )✧`\n",
        "\n",
        "а ещё здесь ваши общие впечатления о домашней работе 🙏🏻"
      ],
      "metadata": {
        "id": "qLvlm3Qqrxs_"
      }
    }
  ]
}